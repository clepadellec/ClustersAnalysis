library(ClustersAnalysis)
installed.packages("ggcorrplot")
library(ggcorrplot)
installed.packages("ggplot2")
library(ggplot2)
installed.packages("ggcorrplot")
install.packages("ggcorrplot")
library(ggcorrplot)
corr <- round(cor(mtcars), 1)
ggcorrplot(corr, hc.order = TRUE, type = "lower",
lab = TRUE))
ggcorrplot(corr, hc.order = TRUE, type = "lower",lab = TRUE))
ggcorrplot(corr, hc.order = TRUE, type = "lower",lab = TRUE)
viz_corr<- function(df){
corr <- round(cor(df), 1)
return(ggcorrplot(corr, hc.order = TRUE, type = "lower",lab = TRUE))
}
viz_corr(mtcars)
library(ClustersAnalysis)
viz_corr(mtcars)
install.packages("ggcorrplot")
library(ClustersAnalysis)
library(readxl)
setwd("C:/Users/DELL/Desktop/Master SISE/Clustering/Exercice 2")
seg=read_excel("segmentation.xlsx", sheet = 1, col_names = T)
# Nombre d'observation.
print(nrow(seg))
# Nombre de variable (active+illustrative).
print(ncol(seg))
# Separer les variables actives.
D1=seg[,1:18]
#print(summary(D1))
# Centrer et reduire les variables actives.
D1_cr=scale(D1,center = T, scale = T)
# K-means pour k varie entre 2 et 10.
inertie=rep(0,10)
for (k in 2:10){
k_mean=kmeans(D1_cr,centers = k, nstart = 5)
inertie[k]=k_mean$betweenss/k_mean$totss
}
plot(1:10, inertie, type="b", xlab = "Nombre de classe", ylab = "Indice de silhouette")
#  K'=5 est le bon nombre K* de classes de ce point de vue: A partir de k=5, l'augmentation de la pourcentage d'inertie explique par la partition est negligeable.
print(inertie)
# Indice silhouette est une mesure de la qulite d'une classification automatique.Pour un individu i fixe, son indice silhouette (sil(i)) est la difference entre la distance moyenne avec les individus de la meme classe et la distance moyenne avec les individus d'une autre classe le plus proche. Indice silhouette de chaque individu joue le meme role qu'avec le cofficient R2 du exercice 1 (sil(i) proche de 1 <---> R2 proche de 1, sil(i) proche de -1 <---> R2 tres negative).
# L'indice de silhouette du model (SIL) de la classification est la moyenne de la (moyenne des indices silhouette des individus de chaque groupe). Donc, on a SIL varie entre -1 et 1. Le model est de bonne qualite si SIL proche de 1 et l'inverse, le model est mauvais si SIL proche de -1.
library(fpc)
sil=kmeansruns(D1_cr,krange = 2:10,criterion = "asw")
plot(1:10, sil$crit, xlab="Nombre de classe", ylab = "Indice de silhouette", type = "b")
# On choisit K''=2: a partir de k=2, l'indice de silhouette n'augmente tres negligeable.
# Kmeans avec 2 classes
D1_kmean2=kmeans(D1_cr,centers = 2, nstart = 5)
# Croisez les groupes avec la variable illustrative IMAGE.
print(table(D1_kmean2$cluster,seg$image))
# On constate que le groupe 1 contient des images: brickface, foliage, grass , window. Le groupe 2 contient des images: path et sky.
# Kmeans avec 5 classes
D1_kmean5=kmeans(D1_cr,centers = 5, nstart = 5)
# Croisez les groupes avec la variable illustrative IMAGE.
print(table(D1_kmean5$cluster,seg$image))
# On constate que: groupe 1(grass), groupe 2(brickface, foliage, window), groupe 3(foliage), groupe 4(skype), groupe 5(cement, path).  La partition en 5 groupes qui paraît la plus adaptée.
set.seed(10)
# Kmeans avec 15 classes
D1_kmean15=kmeans(D1_cr, centers = 15, iter.max = 40, algorithm = "MacQueen")
# Les clusters
clusters=factor(D1_kmean15$cluster)
idclust=table(clusters)
# Distance entre les centres des groupes
distance=dist(D1_kmean15$centers)
#print(distance)
# CAH sur les clusters
library(stats)
D1_cah=hclust(distance, method = "ward.D2", members = idclust)
plot(D1_cah)
# Vu le dendrograme, on peut couper en 2,3 ou encore 5 groupe.
# couper en deux groupes
group=cutree(D1_cah,k=2)
#print(group)
#print(clusters)
D1_cah_classe2=rep(0,nrow(D1))
for ( i in 1:nlevels(clusters)){
D1_cah_classe2[clusters==i]=group[i]
}
# Croiser avec la repartition du kmean avec k=2
table(D1_kmean2$cluster,D1_cah_classe2)
# couper en 5 groupes
group5=cutree(D1_cah,k=5)
D1_cah_classe5=rep(0,nrow(D1))
for ( i in 1:nlevels(clusters)){
D1_cah_classe5[clusters==i]=group5[i]
}
# Croiser avec la repartition du kmean avec k=5
table(D1_kmean5$cluster,D1_cah_classe5)
# ACP sur D1.
D1_acp=princomp(D1, cor = T, scores = T)
# Les valeurs propres
valeurp=D1_acp$sdev^2
print(valeurp)
# Si on applique le regle de Kaiser, on conserve 4 facteurs.
# Scree plot
plot(1:18, valeurp, xlab="Nombre de facteurs", ylab = "Valeur propre", main = "Scree plot", type="b")
# a la vu du graphique, on conserve 2,3 ou 4 facteurs (deux coudes en k=2 et k=4).
# Pourcentage cumule
pourcent=valeurp/sum(valeurp)
pourcent_cum=cumsum(pourcent)
plot(0:18, c(0,pourcent_cum), xlab="Nombre de facteur", ylab = "Pourcentage cumule", main = "Graphique des pourcentages cumules", type="b")
# a la vu de graphique des pourcentages, on decide de prendre 3 facteurs.
# Finalement, il y a 3 facteur retenus.
# Representation des individus sur les deux premiers axes.
plot(D1_acp$scores[,1], D1_acp$scores[,2], col=D1_cah_classe5+1, xlim = c(-4,7), ylim=c(-30,1), xlab="Comp1", ylab="Comp2")
# On distingue le groupe 3 (green) avec les autres groupes en utilisant les coordonnes du premier axe. On distingue le groupe 5 (violet) avec les autres groupes en utilisant les coordonnes du deuxieme axe.
# Representation des individus sur les axes 1-3.
plot(D1_acp$scores[,1], D1_acp$scores[,3], col=D1_cah_classe5+1, xlim = c(-3,7), ylim=c(-5,5), xlab="Comp1", ylab="Comp3")
# On distingue le groupe 2 (blue) avec les autres groupes en utilisant les coordonnes du troisiem axe. On peut distinguer le groupe 1 et group 4 en utilisant les coordonnes du premier axe (positive pour group 1 et negative pour groupe 4, la frontiere est autour du 0)
# Representation des individus sur les axes 2-3.
plot(D1_acp$scores[,2], D1_acp$scores[,3], col=D1_cah_classe5+1, xlim = c(-30,2), ylim=c(-3,5), xlab="Comp2", ylab="Comp3")
table(D1_cah_classe5,seg$image)
# Recherche des variables qui nous permet de distiguer les groupes en utilisant arbre de decision.
P1=cbind(D1,D1_cah_classe5)
library(rpart)
P1_arb=rpart(D1_cah_classe5~., data = P1, method = "class")
library(rpart.plot)
rpart.plot(P1_arb)
P1_predict=predict(P1_arb, type = "class", P1)
# matrice de confusion
table=table(P1$D1_cah_classe5,P1_predict)
# taux d'erreur
print(1-sum(diag(table))/sum(table))
# Reduire la taille de l'arbre
para=rpart.control(minsplit = 100, minbucket = 50)
P1_arb2=rpart(D1_cah_classe5~., data = P1, method = "class", control = para)
rpart.plot(P1_arb2)
P1_predict2=predict(P1_arb2, type = "class", P1)
# matrice de confusion
table2=table(P1$D1_cah_classe5,P1_predict2)
# taux d'erreur
print(1-sum(diag(table2))/sum(table2))
# les variables déterminantes  pour la désignation des groupes sont: rawred.mean, intensity.mean, hue.mean, hedge.mean.
matrix(0,ncol=3,nrow = 4)
library(ClustersAnalysis)
X=matrix(c(1,2,3,4,5,4,3,2,1,3), nrow = 5, byrow = T)
library(ClustersAnalysis)
library(ClustersAnalysis)
X=matrix(c(1,2,3,4,5,4,3,2,1,3), nrow = 5, byrow = T)
matrix_distance(X)
X
sqrt(8)
abs(-1)
library(ClustersAnalysis)
X
matrix_distance(X)
matrix_distance(X,"L1")
matrix_distance(X,"euclidean")
c=factor(1,2,1,1,2)
tapply(X[1,],c, mean)
c
c=factor(c(1,2,1,1,2))
c
tapply(X[1,],c, mean)
matrix=matrix_distance(X,"L1")
tapply(matrix[1,],c, mean)
tapply(matrix[1,-1],c, mean)
tapply(matrix[1,-1],c[-1], mean)
a=tapply(matrix[1,-1],c[-1], mean)
b=tapply(matrix[1,-1],c[-1], mean)
rbind(a,b)
cbind(tapply(matrix[1,-1],c[-1], mean),tapply(matrix[1,-1],c[-1], mean))
rbind(tapply(matrix[1,-1],c[-1], mean),tapply(matrix[1,-1],c[-1], mean))
a
b=[]
b=c()
rbind(a,b)
library(ClustersAnalysis)
A=matrix_distance(X,"L1")
c
mean_distance(A,c)
A
library(ClustersAnalysis)
A=matrix_distance(X,"L1")
c
mean_distance(A,c)
library(ClustersAnalysis)
mean_distance(A,c)
library(ClustersAnalysis)
mean_distance(A,c)
mean_distance(A,c)
c=factor(c(1,2,1,1,2))
c
t=table(c)
t
t[1]
library(ClustersAnalysis)
mean_distance(A,c)
A=matrix_distance(X,"L1")
X=matrix(c(1,2,3,4,5,4,3,2,1,3), nrow = 5, byrow = T)
c=factor(c(1,2,1,1,2))
mean_distance(A,c)
A=matrix_distance(X,"L1")
mean_distance(A,c)
B=mean_distance(A,c)
B[1,]
B[1,]['1']
X=matrix(c(1,2,3,4,5,4,3,2,1,3), nrow = 5, byrow = T)
c=factor(c(a,b,a,a,b))
c=factor(c("a","b","a","a","b"))
A=matrix_distance(X,"L1")
B=mean_distance(A,c)
B
B[1,]['b']
B[1,]['2']
B[1,]['a']
as.numeric(B[1,]['a'])
c[1]
str(c[1])
as.character(c[1])
as.numeric(B[1,])
B[1,][-'a']
B[1,]['a']
c=="a"
c!="a"
B[1,]
B[1,colnames(B)!=b]
B[1,colnames(B)!="b"]
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
c
X
library(ClustersAnalysis)
library(ClustersAnalysis)
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
X
c
matrix_distance(X,"L1")
moy=mean_distance(matrix_distance(X,"L1"),c)
moy
as.numeric(moy[1,][as.character(c[i])])
as.numeric(moy[1,][as.character(c[1])])
min(as.numeric(moy[1,colnames(moy)!=as.character(c[1])]))
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
moy
nrow(moy)
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
c=factor(c("a","a","a","a","a"))
silhouette_ind(X,c,"L1")
matrix_distance(X,"L1")
mean_distance(matrix_distance(X,"L1"),c)
library(ClustersAnalysis)
silhouette_ind(X,c,"L1")
data("mtcars")
viz_corr(mtcars)
silhouette_ind(X,c,"L1")
c=factor(c("a","b","a","a","b"))
silhouette_ind(X,c,"L1")
data.frame({"a":c(1,2)})
data.frame("a"=c(1,2))
data.frame("a"=c)
silhouette_ind(X,c,"L1")
sil=silhouette_ind(X,c,"L1")
data=data.frame("sil"=sil,"labels"=c)
data
library(ggplot2)
ggplot(data, aes(sil, color="labels"))+geom_bar()
ggplot(data, aes(y=sil, color="labels"))+geom_bar()
ggplot(data, aes(y=sil, color=labels))+geom_bar()
data=data.frame("sil"=sil,"labels"=c, "indice"=c(1,2,3,4,5))
ggplot(data, aes(indice,sil, color=labels))+geom_bar()
ggplot(data, aes(indice,sil, col=labels))+geom_bar()
ggplot(data, aes(indice,sil, col=labels))+geom_bar()
data
ggplot(data, aes(indice,sil))+ geom_point()
ggplot(data, aes(indice,sil))+ geom_bar()
ggplot(data, aes(sil)) +geom_bar(position = 'dodge') + labs(x = "Number of Cylinders", y = "Count")
ggplot(indice, aes(sil)) +geom_bar(position = 'dodge') + labs(x = "Number of Cylinders", y = "Count")
ggplot(data, aes(indice,sil)) +geom_bar(position = 'dodge') + labs(x = "Number of Cylinders", y = "Count")
ggplot(data, aes(y=sil)) +geom_bar(position = 'dodge') + labs(x = "Number of Cylinders", y = "Count")
ggplot(data, aes(indice,sil,color=labels))+ geom_line()
ggplot(data, aes(sil,indice,color=labels))+ geom_bar()
ggplot(data, aes(sil,indice,color=labels))+ geom_line()
ggplot(data, aes(sil,indice,color=labels))+ geom_bar()
data=data.frame("sil"=sil,"labels"=c, "indice"=c("1","2","3","4","5"))
ggplot(data, aes(sil,indice,color=labels))+ geom_bar()
ggplot(data, aes(indice,sil,color=labels))+ geom_bar()
ggplot(data, aes(indice,sil,color=labels))+ geom_area()
library(ClustersAnalysis)
data=data.frame("sil"=sil,"labels"=c, "indice"=c(1,2,3,4,5))
data=data.frame("sil"=c(0.1,0.5,0.3,0.6,0.8,0.1,0.8,0.7,0.5,0.4),"labels"=c(1,2,1,1,2,1,2,1,2,1), "indice"=c("1","2","3","4","5","6","7","8","9","10"))
ggplot(data, aes(indice,sil,color=labels))+ geom_bar()
library(ggplot2)
ggplot(data, aes(indice,sil,color=labels))+ geom_bar()
ggplot(data, aes(indice,sil,color=labels))+ geom_bar(stat = "count")
ggplot(data)+geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1), stat = "count")
ggplot(data)+geom_bar(mapping = aes(x = indice, y = sil, group = 1), stat = "count")
install.packages("factoextra")
library(ClustersAnalysis)
data("USArrests")
y
y=c(rep(1,20),rep(2,10), rep(3,20))
sil_pca_plot(USArrests,y,1,2)
sil_pca_plot(USArrests,y,i=1,j=2)
y=factor(c(rep(1,20),rep(2,10), rep(3,20)))
sil_pca_plot(USArrests,y,i=1,j=2)
dclass(y)
class(y)
library(ClustersAnalysis)
y=c(rep(1,20),rep(2,10), rep(3,20))
sil_pca_plot(USArrests,y,i=1,j=2)
y=factor(c(rep(1,20),rep(2,10), rep(3,20)))
sil_pca_plot(USArrests,y,i=1,j=2)
par(mfrow=c(3,1))
modele=list(ar=c(0.8))
ar1=arima.sim(modele,1000)
plot.ts(ar1)
acf(ar1)
pacf(ar1)
par(mfrow=c(3,1))
modele=list(ma=c(0.8))
ma1=arima.sim(model,1000)
par(mfrow=c(3,1))
modele=list(ma=c(0.8))
ma1=arima.sim(modele,1000)
plot.ts(ma1)
acf(am1)
par(mfrow=c(3,1))
modele=list(ma=c(0.8))
am1=arima.sim(modele,1000)
plot.ts(ma1)
acf(am1)
pacf(am1)
install.packages("fpp2")
